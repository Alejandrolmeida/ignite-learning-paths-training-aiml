{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License.\n",
    "\n",
    "### Intel NLP-Architect ABSA on AzureML \n",
    "\n",
    "### INSTRUCTOR VERSION\n",
    "\n",
    "> **This instructor version of the notebook gives additional instructions as to which cells should be run in demo mode, and which should not. It assumes that before the demo you will execute the complete notebook, and then during the demo certain cells would be re-run to demonstrate working process.**\n",
    "\n",
    "This notebook contains an end-to-end walkthrough of using Azure Machine Learning Service to train, finetune and test [Aspect Based Sentiment Analysis Models using Intel's NLP Architect](http://nlp_architect.nervanasys.com/absa.html)\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "* Understand the architecture and terms introduced by Azure Machine Learning (AML)\n",
    "* Have working Jupyter Notebook Environment. You can:\n",
    "    - Install Python environment locally, as described below in **Local Installation**\n",
    "    - Use [Azure Notebooks](https://docs.microsoft.com/ru-ru/azure/notebooks/azure-notebooks-overview/?wt.mc_id=absa-notebook-abornst). In this case you should upload the `absa.ipynb` file to a new Azure Notebooks project, or just clone the [GitHub Repo](https://github.com/microsoft/ignite-learning-paths/tree/master/aiml/aiml40).\n",
    "* Azure Machine Learning Workspace in your Azure Subscription\n",
    "\n",
    "#### Local Installation\n",
    "\n",
    "Install the Python SDK: make sure to install notebook, and contrib:\n",
    "\n",
    "```shell\n",
    "conda create -n azureml -y Python=3.6\n",
    "source activate azureml\n",
    "pip install --upgrade azureml-sdk[notebooks,contrib] \n",
    "conda install ipywidgets\n",
    "jupyter nbextension install --py --user azureml.widgets\n",
    "jupyter nbextension enable azureml.widgets --user --py\n",
    "```\n",
    "\n",
    "You will need to restart jupyter after this Detailed instructions are [here](https://docs.microsoft.com/en-us/azure/machine-learning/service/quickstart-create-workspace-with-python/?WT.mc_id=absa-notebook-abornst)\n",
    "\n",
    "If you need a free trial account to get started you can get one [here](https://azure.microsoft.com/en-us/offers/ms-azr-0044p/?WT.mc_id=absa-notebook-abornst)\n",
    "\n",
    "#### Creating Azure ML Workspace\n",
    "\n",
    "Azure ML Workspace can be created by using one of the following ways:\n",
    "* Manually through [Azure Portal](http://portal.azure.com/?WT.mc_id=absa-notebook-abornst) - [here is the complete walkthrough](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-manage-workspace/?wt.mc_id=absa-notebook-abornst)\n",
    "* Using [Azure CLI](https://docs.microsoft.com/ru-ru/cli/azure/?view=azure-cli-latest&wt.mc_id=absa-notebook-abornst), using the following commands:\n",
    "\n",
    "```shell\n",
    "az extension add -n azure-cli-ml\n",
    "az group create -n absa -l westus2\n",
    "az ml workspace create -w absa_space -g absa\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize workspace\n",
    "\n",
    "To access an Azure ML Workspace, you will need to import the AML library and the following information:\n",
    "* A name for your workspace (in our example - `absa_space`)\n",
    "* Your subscription id (can be obtained by running `az account list`)\n",
    "* The resource group name (in our case `absa`)\n",
    "\n",
    "Initialize a [Workspace](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace/?WT.mc_id=absa-notebook-abornst) object from the existing workspace you created in the Prerequisites step or create a new one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **This cell can be run without problem, because it will just create a connection object for the workspace. Make sure to insert the correct `subscription_id` value before use, or have `config.json` file ready.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abla_space\twesteurope\tabla\twesteurope\n",
      "Library configuration succeeded\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "#subscription_id = ''\n",
    "#resource_group  = 'absa'\n",
    "#workspace_name  = 'absa_space'\n",
    "#ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n",
    "#ws.write_config()\n",
    "\n",
    "try:\n",
    "    ws = Workspace.from_config()\n",
    "    print(ws.name, ws.location, ws.resource_group, ws.location, sep='\\t')\n",
    "    print('Library configuration succeeded')\n",
    "except:\n",
    "    print('Workspace not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two computer option run once(preview) and persistent compute for this demo we will use persistent compute to learn more about run once compute check out the [docs](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-set-up-training-targets#amlcompute?WT.mc_id=absa-notebook-abornst)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **This cell can be run because it will not re-create a cluster. Although it does not make much sense to run it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing cluster, use it.\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "Minimum number of nodes requested have been provisioned\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Choose a name for your CPU cluster\n",
    "cluster_name = \"absa-cluster\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    cluster = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D3_V2',\n",
    "                                                           vm_priority='lowpriority',\n",
    "                                                           min_nodes=1,\n",
    "                                                           max_nodes=1)\n",
    "    cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Data\n",
    "\n",
    "The dataset we are using comes from trip advisor and is in the open domain, this can be replaced with any csv file with rows of text as the absa model is unsupervised. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **You do not need to re-run this code during demo, as the file will be already downloaded**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-09-20 10:26:47--  https://raw.githubusercontent.com/NervanaSystems/nlp-architect/master/datasets/absa/tripadvisor_co_uk-travel_restaurant_reviews_sample_2000_train.csv\n",
      "Resolving webproxy (webproxy)... 10.36.6.1\n",
      "Connecting to webproxy (webproxy)|10.36.6.1|:3128... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 961388 (939K) [text/plain]\n",
      "Saving to: ‘tripadvisor_co_uk-travel_restaurant_reviews_sample_2000_train.csv’\n",
      "\n",
      "tripadvisor_co_uk-t 100%[===================>] 938.86K  2.39MB/s    in 0.4s    \n",
      "\n",
      "2019-09-20 10:26:48 (2.39 MB/s) - ‘tripadvisor_co_uk-travel_restaurant_reviews_sample_2000_train.csv’ saved [961388/961388]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/NervanaSystems/nlp-architect/master/datasets/absa/tripadvisor_co_uk-travel_restaurant_reviews_sample_2000_train.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **You do not need to re-run this code during demo as the file will be already uploaded**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os                            \n",
    "lib_root = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "ds = ws.get_default_datastore()\n",
    "ds. upload_files([os.path.join(lib_root,'tripadvisor_co_uk-travel_restaurant_reviews_sample_2000_train.csv')], \n",
    "                relative_root=lib_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **It does not matter if you execute this cell or not, because it will just overwrite the file. You may execute it, just to make the demo more live**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "import argparse\n",
    "import os \n",
    "from azureml.core import Run\n",
    "from spacy.cli.download import download as spacy_download\n",
    "from nlp_architect.models.absa.train.train import TrainSentiment\n",
    "from nlp_architect.models.absa import TRAIN_OUT\n",
    "from nlp_architect.utils.io import download_unzip\n",
    "\n",
    "spacy_download('en')\n",
    "EMBEDDING_URL = 'http://nlp.stanford.edu/data', 'glove.840B.300d.zip'\n",
    "EMBEDDING_PATH = TRAIN_OUT / 'word_emb_unzipped' / 'glove.840B.300d.txt'\n",
    "download_unzip(*EMBEDDING_URL, EMBEDDING_PATH)\n",
    "\n",
    "parser = argparse.ArgumentParser(description='ABSA Train')\n",
    "parser.add_argument('--data_folder', type=str, dest='data_folder', help='data folder mounting point')\n",
    "parser.add_argument('--learning_rate', type=float, default=3e-5, help='learning rate')\n",
    "parser.add_argument('--epochs', type=int, default=5)\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "rerank_model = None # Path to rerank model .h5 file\n",
    "parsed_data = None\n",
    "\n",
    "tripadvisor_train = os.path.join(args.data_folder, \n",
    "                                 'tripadvisor_co_uk-travel_restaurant_reviews_sample_2000_train.csv')\n",
    "\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "    \n",
    "\n",
    "train = TrainSentiment(parse=not parsed_data, rerank_model=rerank_model)\n",
    "\n",
    "\n",
    "opinion_lex, aspect_lex = train.run(data=tripadvisor_train,\n",
    "                                    out_dir = './outputs',\n",
    "                                    parsed_data=parsed_data)\n",
    "\n",
    "# get hold of the current run\n",
    "run = Run.get_context()\n",
    "\n",
    "run.log('Aspect Lexicon Size:', len(aspect_lex))\n",
    "run.log('Opinion Lexicon Size:', len(opinion_lex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create An Expierment\n",
    "\n",
    "Create an [Experiment](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#experiment/?WT.mc_id=absa-notebook-abornst) to track all the runs in your workspace for this distributed PyTorch tutorial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **In most of the cases, you want to skip the following 3 cells during the demo, in order not to run the experiment again. However, you may also start another experiment if time permists, in which case you can run them**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "experiment_name = 'absa'\n",
    "\n",
    "exp = Experiment(workspace=ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.estimator import Estimator\n",
    "\n",
    "script_params = {\n",
    "    '--data_folder': ds,\n",
    "}\n",
    "\n",
    "# find a way to integrate nlp architect \n",
    "nlp_est = Estimator(source_directory='.',\n",
    "                   script_params=script_params,\n",
    "                   compute_target=cluster,\n",
    "                   environment_variables = {'NLP_ARCHITECT_BE':'CPU'},\n",
    "                   entry_script='train.py',\n",
    "                   pip_packages=['git+https://github.com/NervanaSystems/nlp-architect.git@absa'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'absa_1568985331_df076c3c'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run = exp.submit(nlp_est)\n",
    "run_id = run.id\n",
    "print(run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **To retrieve the run, we use run id here. It can either be hard-coded from the previous pre-demo run, or you can rely on the jupyter kernel not restarting, in which case it will be saved in the `run_id` variable. So, if the jupyter engine has not been restarted, you may run cell 2, otherwise run cell 1** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = [r for r in exp.get_runs() if r.id == 'absa_1568985331_df076c3c'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = [r for r in exp.get_runs() if r.id == run_id][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Run this to show the result of the run, either in progress or completed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e52608b49f274a5a804b95af28165038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning NLP Archictect  with AzureML HyperDrive\n",
    "Although ABSA is an unsupervised method it can be fined tuned if provided with a small sample of labeled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **It probably makes sense to skip the whole hyperdrive section, and just go through the code overview**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HyperDriveRunConfig is deprecated. Please use the new HyperDriveConfig class.\n"
     ]
    }
   ],
   "source": [
    "from azureml.train.hyperdrive import *\n",
    "import math\n",
    "\n",
    "param_sampling = RandomParameterSampling( {\n",
    "        'asp_thresh': list(range(1,5)),\n",
    "         'op_thresh': 2, \n",
    "         'max_iter': list(range(1,5))\n",
    "    }\n",
    ")\n",
    "\n",
    "hyperdrive_run_config = HyperDriveRunConfig(estimator=nlp_est,\n",
    "                                            hyperparameter_sampling=param_sampling, \n",
    "                                            primary_metric_name='f1', # This requires a modification of script to finetune on supervised data\n",
    "                                            primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n",
    "                                            max_total_runs=16,\n",
    "                                            max_concurrent_runs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lauch the hyperparameter tuning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(workspace=ws, name='hyperdrive')\n",
    "hyperdrive_run = experiment.submit(hyperdrive_run_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor HyperDrive runs\n",
    "We can monitor the progress of the runs with the following Jupyter widget. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "675d725b1e64421c94d55fe640f35f84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_HyperDriveWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1046ca3945a947a28601ebcf88b0e153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "\n",
    "RunDetails(hyperdrive_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find and register the best model\n",
    "Once all the runs complete, we can find the run that produced the model with the highest evaluation (METRIC TBD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run = hyperdrive_run.get_best_run_by_primary_metric()\n",
    "best_run_metrics = best_run.get_metrics()\n",
    "print(best_run)\n",
    "print('Best Run is:\\n  F1: {0:.5f} \\n  Learning rate: {1:.8f}'.format(\n",
    "        best_run_metrics['eval_f1'][-1],\n",
    "        best_run_metrics['lr']\n",
    "     ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Model Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect_lex = run.register_model(model_name='aspect_lex', model_path='outputs/train_out/generated_aspect_lex.csv')\n",
    "opinion_lex = run.register_model(model_name='opinion_lex', model_path='outputs/train_out/generated_opinion_lex_reranked.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy as web service\n",
    "Once you've tested the model and are satisfied with the results, deploy the model as a web service hosted in [Azure Container Instances](https://azure.microsoft.com/en-us/services/container-instances/?WT.mc_id=bert-notebook-abornst).\n",
    "\n",
    "To build the correct environment for ACI, provide the following:\n",
    "\n",
    "A scoring script to show how to use the model\n",
    "An environment file to show what packages need to be installed\n",
    "A configuration file to build the ACI\n",
    "The model you trained before\n",
    "\n",
    "## Create scoring script\n",
    "Create the scoring script, called score.py, used by the web service call to show how to use the model.\n",
    "\n",
    "You must include two required functions into the scoring script:\n",
    "\n",
    "The init() function, which typically loads the model into a global object. This function is run only once when the Docker container is started.\n",
    "\n",
    "The run(input_data) function uses the model to predict a value based on the input data. Inputs and outputs to the run typically use JSON for serialization and de-serialization, but other formats are supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile score.py\n",
    "from azureml.core.model import Model\n",
    "from nlp_architect.models.absa.inference.inference import SentimentInference\n",
    "from spacy.cli.download import download as spacy_download\n",
    "\n",
    "\n",
    "def init():\n",
    "    \"\"\"\n",
    "    Set up the ABSA model for Inference  \n",
    "    \"\"\"\n",
    "    global inference\n",
    "    spacy_download('en')\n",
    "    aspect_lex = Model.get_model_path('aspect_lex')\n",
    "    opinion_lex = Model.get_model_path('opinion_lex')    \n",
    "    inference = SentimentInference(aspect_lex, opinion_lex)\n",
    "\n",
    "def run(raw_data):\n",
    "    \"\"\"\n",
    "    Evaluate the model and return JSON string\n",
    "    \"\"\"\n",
    "    sentiment_doc = inference.run(doc=raw_data)\n",
    "    return sentiment_doc.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create configuration files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACI Config\n",
    "Create a ACI configuration file and specify the number of CPUs and gigabyte of RAM needed for your ACI container. While it depends on your model, the default of 1 core and 1 gigabyte of RAM is usually sufficient for many models. If you feel you need more later, you would have to recreate the image and redeploy the service.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n",
    "                                               memory_gb=1,  \n",
    "                                               tags={\"data\": \"text\",  \"method\" : \"NLP Architcet ABSA\"}, \n",
    "                                               description='Predict ABSA with NLP Architect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Enviorment File\n",
    "create an environment file, called myenv.yml, that specifies all of the script's package dependencies. This file is used to ensure that all of those dependencies are installed in the Docker image. This model needs nlp-architect and the azureml-sdk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.conda_dependencies import CondaDependencies \n",
    "\n",
    "pip = [\"azureml-defaults\", \"azureml-monitoring\", \"git+https://github.com/NervanaSystems/nlp-architect.git@absa\"]\n",
    "\n",
    "myenv = CondaDependencies.create(pip_packages=pip)\n",
    "\n",
    "with open(\"myenv.yml\",\"w\") as f:\n",
    "    f.write(myenv.serialize_to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Environment Config\n",
    "Create a Enviorment configuration file and specify the enviroment and enviormental variables required for the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "deploy_env = Environment.from_conda_specification('absa_env', \"myenv.yml\")\n",
    "deploy_env.environment_variables={'NLP_ARCHITECT_BE': 'CPU'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Config \n",
    "Create an inference configuration that recieves the deployment enviorment and the entry script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import InferenceConfig\n",
    "inference_config = InferenceConfig(environment=deploy_env,\n",
    "                                   entry_script=\"score.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy in ACI\n",
    "Estimated time to complete: about 7-8 minutes\n",
    "\n",
    "Configure the image and deploy. The following code goes through these steps:\n",
    "\n",
    "Build an image using:\n",
    "The scoring file (score.py)\n",
    "The environment file (myenv.yml)\n",
    "The model file\n",
    "Register that image under the workspace.\n",
    "Send the image to the ACI container.\n",
    "Start up a container in ACI using the image.\n",
    "Get the web service HTTP endpoint.\n",
    "https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-deploy-azure-container-instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating service\n",
      "Running.........................................\n",
      "SucceededACI service creation operation finished, operation \"Succeeded\"\n",
      "Healthy\n",
      "CPU times: user 397 ms, sys: 69.6 ms, total: 466 ms\n",
      "Wall time: 3min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from azureml.core.webservice import AciWebservice, Webservice\n",
    "from azureml.core.model import Model\n",
    "\n",
    "aspect_lex = Model(ws, 'aspect_lex')\n",
    "opinion_lex = Model(ws, 'opinion_lex')    \n",
    "\n",
    "service = Model.deploy(workspace=ws,\n",
    "                       name='absa-srvc', \n",
    "                       models=[aspect_lex, opinion_lex],\n",
    "                       inference_config=inference_config, \n",
    "                       deployment_config=aciconfig)\n",
    "service.wait_for_deployment(show_output = True)\n",
    "print(service.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View service logs: This is powerful for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /azureml-envs/azureml_e748c621598b8a5948a2f7276c7bb60c/lib/libtinfo.so.5: no version information available (required by /bin/bash)\n",
      "/bin/bash: /azureml-envs/azureml_e748c621598b8a5948a2f7276c7bb60c/lib/libtinfo.so.5: no version information available (required by /bin/bash)\n",
      "/bin/bash: /azureml-envs/azureml_e748c621598b8a5948a2f7276c7bb60c/lib/libtinfo.so.5: no version information available (required by /bin/bash)\n",
      "/bin/bash: /azureml-envs/azureml_e748c621598b8a5948a2f7276c7bb60c/lib/libtinfo.so.5: no version information available (required by /bin/bash)\n",
      "2019-08-26T13:07:50,262247639+00:00 - gunicorn/run \n",
      "2019-08-26T13:07:50,262797544+00:00 - iot-server/run \n",
      "2019-08-26T13:07:50,262247539+00:00 - rsyslog/run \n",
      "2019-08-26T13:07:50,263256549+00:00 - nginx/run \n",
      "bash: /azureml-envs/azureml_e748c621598b8a5948a2f7276c7bb60c/lib/libtinfo.so.5: no version information available (required by bash)\n",
      "/usr/sbin/nginx: /azureml-envs/azureml_e748c621598b8a5948a2f7276c7bb60c/lib/libcrypto.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n",
      "/usr/sbin/nginx: /azureml-envs/azureml_e748c621598b8a5948a2f7276c7bb60c/lib/libcrypto.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n",
      "/usr/sbin/nginx: /azureml-envs/azureml_e748c621598b8a5948a2f7276c7bb60c/lib/libssl.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n",
      "/usr/sbin/nginx: /azureml-envs/azureml_e748c621598b8a5948a2f7276c7bb60c/lib/libssl.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n",
      "/usr/sbin/nginx: /azureml-envs/azureml_e748c621598b8a5948a2f7276c7bb60c/lib/libssl.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n",
      "Starting gunicorn 19.9.0\n",
      "Listening at: http://127.0.0.1:31311 (12)\n",
      "Using worker: sync\n",
      "worker timeout is set to 300\n",
      "Booting worker with pid: 37\n",
      "EdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n",
      "/bin/bash: /azureml-envs/azureml_e748c621598b8a5948a2f7276c7bb60c/lib/libtinfo.so.5: no version information available (required by /bin/bash)\n",
      "2019-08-26T13:07:55,845199922+00:00 - iot-server/finish 1 0\n",
      "2019-08-26T13:07:55,846288933+00:00 - Exit code 1 is normal. Not restarting iot-server.\n",
      "Initializing logger\n",
      "Starting up app insights client\n",
      "Starting up request id generator\n",
      "Starting up app insight hooks\n",
      "Invoking user's init function\n",
      "Collecting en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz (11.1MB)\n",
      "Building wheels for collected packages: en-core-web-sm\n",
      "  Building wheel for en-core-web-sm (setup.py): started\n",
      "  Building wheel for en-core-web-sm (setup.py): finished with status 'done'\n",
      "  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.1.0-cp36-none-any.whl size=11074435 sha256=57529de5476488be86a3aacd1d1228af49ffcfd106e71516c94f39750dbd060a\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-dylq_y2m/wheels/39/ea/3b/507f7df78be8631a7a3d7090962194cf55bc1158572c0be77f\n",
      "Successfully built en-core-web-sm\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-2.1.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/azureml-envs/azureml_e748c621598b8a5948a2f7276c7bb60c/lib/python3.6/site-packages/en_core_web_sm\n",
      "-->\n",
      "/azureml-envs/azureml_e748c621598b8a5948a2f7276c7bb60c/lib/python3.6/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n",
      "2019-08-26 13:08:04,646 | azureml.core.run | DEBUG | Could not load run context RunEnvironmentException:\n",
      "\tMessage: Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.\n",
      "\tInnerException None\n",
      "\tErrorResponse {\"error\": {\"message\": \"Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.\"}}, switching offline: False\n",
      "2019-08-26 13:08:04,647 | azureml.core.run | DEBUG | Could not load the run context and allow_offline set to False\n",
      "2019-08-26 13:08:04,647 | azureml.core.model | DEBUG | RunEnvironmentException: RunEnvironmentException:\n",
      "\tMessage: Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.\n",
      "\tInnerException RunEnvironmentException:\n",
      "\tMessage: Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.\n",
      "\tInnerException None\n",
      "\tErrorResponse {\"error\": {\"message\": \"Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.\"}}\n",
      "\tErrorResponse {\"error\": {\"message\": \"Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.\"}}\n",
      "2019-08-26 13:08:04,647 | azureml.core.model | DEBUG | version is None. Latest version is 2\n",
      "2019-08-26 13:08:04,647 | azureml.core.model | DEBUG | Found model path at azureml-models/aspect_lex/2/generated_aspect_lex.csv\n",
      "2019-08-26 13:08:04,647 | azureml.core.run | DEBUG | Could not load run context RunEnvironmentException:\n",
      "\tMessage: Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.\n",
      "\tInnerException None\n",
      "\tErrorResponse {\"error\": {\"message\": \"Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.\"}}, switching offline: False\n",
      "2019-08-26 13:08:04,647 | azureml.core.run | DEBUG | Could not load the run context and allow_offline set to False\n",
      "2019-08-26 13:08:04,647 | azureml.core.model | DEBUG | RunEnvironmentException: RunEnvironmentException:\n",
      "\tMessage: Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.\n",
      "\tInnerException RunEnvironmentException:\n",
      "\tMessage: Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.\n",
      "\tInnerException None\n",
      "\tErrorResponse {\"error\": {\"message\": \"Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.\"}}\n",
      "\tErrorResponse {\"error\": {\"message\": \"Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.\"}}\n",
      "2019-08-26 13:08:04,648 | azureml.core.model | DEBUG | version is None. Latest version is 1\n",
      "2019-08-26 13:08:04,648 | azureml.core.model | DEBUG | Found model path at azureml-models/opinion_lex/1/generated_opinion_lex_reranked.csv\n",
      "Using pre-trained BIST model.\n",
      "Downloading pre-trained BIST model...\n",
      "Downloading file to: /root/nlp-architect/cache/bist-pretrained/bist-pretrained.zip\n",
      "  0%|          | 0/24 [00:00<?, ?MB/s]\n",
      "  4%|▍         | 1/24 [00:00<00:08,  2.57MB/s]\n",
      " 12%|█▎        | 3/24 [00:00<00:06,  3.38MB/s]\n",
      " 33%|███▎      | 8/24 [00:00<00:03,  4.63MB/s]\n",
      " 58%|█████▊    | 14/24 [00:00<00:01,  6.28MB/s]\n",
      " 83%|████████▎ | 20/24 [00:01<00:00,  8.38MB/s]\n",
      "25MB [00:01, 22.74MB/s]\n",
      "[dynet] random seed: 2734000164\n",
      "[dynet] allocating memory: 512MB\n",
      "[dynet] memory allocation done.\n",
      "Download Complete\n",
      "Unzipping...\n",
      "Done.\n",
      "Users's init has completed successfully\n",
      "Scoring timeout setting is not found. Use default timeout: 3600000 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(service.get_logs())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "service = ws.webservices['absa-srvc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the scoring web service's HTTP endpoint, which accepts REST client calls. This endpoint can be shared with anyone who wants to test the web service or integrate it into an application.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://401d4329-3c4c-4187-97ec-3cad2d439708.eastus.azurecontainer.io/score\n"
     ]
    }
   ],
   "source": [
    "print(service.scoring_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Deployed ACI Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"_doc_text\": \"The ambiance is charming. Uncharacteristically, the service was DREADFUL.              When we wanted to pay our bill at the end of the evening, our waitress was nowhere to be found...\", \"_sentences\": [{\"_start\": 0, \"_end\": 24, \"_events\": [[{\"_text\": \"ambiance\", \"_type\": \"ASPECT\", \"_polarity\": \"POS\", \"_score\": 1.0, \"_start\": 4, \"_len\": 8}, {\"_text\": \"charming\", \"_type\": \"OPINION\", \"_polarity\": \"POS\", \"_score\": 1.0, \"_start\": 16, \"_len\": 8}]]}, {\"_start\": 26, \"_end\": 72, \"_events\": [[{\"_text\": \"service\", \"_type\": \"ASPECT\", \"_polarity\": \"NEG\", \"_score\": -1.0, \"_start\": 52, \"_len\": 7}, {\"_text\": \"DREADFUL\", \"_type\": \"OPINION\", \"_polarity\": \"NEG\", \"_score\": -1.0, \"_start\": 64, \"_len\": 8}]]}, {\"_start\": 87, \"_end\": 183, \"_events\": [[{\"_text\": \"waitress\", \"_type\": \"ASPECT\", \"_polarity\": \"NEG\", \"_score\": -0.98065746, \"_start\": 149, \"_len\": 8}, {\"_text\": \"waitress\", \"_type\": \"OPINION\", \"_polarity\": \"NEG\", \"_score\": -0.98065746, \"_start\": 149, \"_len\": 8}]]}]}'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from nlp_architect.models.absa.inference.data_types import TermType\n",
    "\n",
    "# send a random row from the test set to score\n",
    "input_data = \"The ambiance is charming. Uncharacteristically, the service was DREADFUL.\\\n",
    "              When we wanted to pay our bill at the end of the evening, our waitress was nowhere to be found...\"\n",
    "\n",
    "headers = {'Content-Type':'application/json'}\n",
    "\n",
    "resp = requests.post(service.scoring_uri, input_data, headers=headers)\n",
    "resp.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Render the response using [Displacy](https://spacy.io/usage/visualizers/)\n",
    "Note ```Spacy``` Must be installed on the local machine for this to work can be installed with ```pip install spacy```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">The \n",
       "<mark class=\"entity\" style=\"background: #7CFC00; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    ambiance\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">POS</span>\n",
       "</mark>\n",
       " is charming. Uncharacteristically, the \n",
       "<mark class=\"entity\" style=\"background: #FF0000; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    service\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">NEG</span>\n",
       "</mark>\n",
       " was DREADFUL.              When we wanted to pay our bill at the end of the evening, our \n",
       "<mark class=\"entity\" style=\"background: #FF0000; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    waitress\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">NEG</span>\n",
       "</mark>\n",
       " was nowhere to be found...</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "if resp.text:\n",
    "    doc = json.loads(resp.json()) # load response as dictionary\n",
    "    doc_viz = {'text':doc[\"_doc_text\"], 'ents':[]}\n",
    "    for s in doc[\"_sentences\"]:\n",
    "        for e in s[\"_events\"][0]:\n",
    "            if e[\"_type\"] == \"ASPECT\":\n",
    "                doc_viz['ents'].append({'start': e[\"_start\"], 'end': e[\"_start\"] + e[\"_len\"], 'label':str(e[\"_polarity\"])})\n",
    "    doc_viz['ents'].sort(key=lambda m: m[\"start\"])\n",
    "    displacy.render(doc_viz, style=\"ent\", options={'colors':{'POS':'#7CFC00', 'NEG':'#FF0000'}}, manual=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
