# <a name="aiml20-speaker-notes"></a>AIML20:講演者向けメモ

https://microsoft.sharepoint.com/:p:/t/CloudDevAdvocacy/EctuTXQCOdpGqc5lhQgUnMgBr-R6hlWR5MuLE3qCIFgoHA?e=r1szwF の PPT プレゼンテーション用メモです

関連するデモ スクリプトについては、 https://github.com/microsoft/ignite-learning-paths/tree/master/aiml/aiml20 を参照してください。 `DEMO%20Setup.md`で開始します。

## <a name="slide-notes"></a>スライド メモ

スライドはタイトルでのみ識別されます。

### <a name="slide-microsft-ignite-the-tour"></a>スライド:Microsoft Ignite the Tour

プレゼンテーション前の導入スライド

### <a name="slide-using-pre-built-ai-to-solve-business-problems"></a>スライド:事前構築済み AI を使用してビジネスの課題を解決する

ようこそ。私は、\<所属の名前\>と申します。 本日は、大量のコンピューティング能力や AI の専門知識を必要とせずに、アプリケーションに人工知能機能を搭載する方法について説明します。
これには、特定のニーズに合わせてカスタマイズすることもできる、クラウドで利用できる既成の AI サービスを使用します。

(スライドの連絡先情報を置き換えてください。)

### <a name="slide-resources"></a>スライド:リソース

本日の発表ではあらゆる情報やリソースへのリンクをご紹介しますが、詳細はすべて、ここに示されているセッション リソース ハブに含まれているため、プレゼンテーション中に確認できなくても大丈夫です。 また、デモもご用意しておりますので、この Github リポジトリでは Azure にワンクリックですべてをデプロイできるボタンを含む完全なソース コードを利用してすべてお試しいただけます。 このスライドはプレゼンテーションの最後にも表示致しますので、ぜひカメラをご用意いただければと思います。

### <a name="slide-adding-humanlike-capabilities-to-apps"></a>スライド:人間的機能をアプリに追加

事前構築された AI サービスを使用してアプリケーションに人間的機能を追加します。それはどのような意味があるのでしょうか。 次に例をいくつか示します。

### <a name="slide-enhance-apps-with-humanlike-capabilities"></a>スライド:人間に似た機能を使用してアプリを強化する

[クリック] チャット インターフェイスを追加するなど、アプリに音声機能を追加できます。
 
[クリック] アプリに画像を見せ、解釈する機能を追加できます。

[クリック] アプリが直感的にユーザーが実行しようとしている可能性が最も高いものを把握し、ユーザー インターフェイスを自動的に最適化することができます。

[クリック] アプリに読み取り機能を追加し、ユーザーの発話言語でコミュニケーションを取ることができます。

[クリック] または、データ ストリームをスキャンするプロセスを自動化して異常を検出し、適切に対処することができます。

これらは、ほんの一例です。

### <a name="slide-overview-of-azure-cognitive-services"></a>スライド:Azure Cognitive Services の概要

ここまで、AI の手法がどのように役立つかについてお話してきました。 しかし、実装するには、大量のデータや技術的な専門知識は必要なのでしょうか。

答えは、いいえです。 シンプルな REST API 呼び出しにより利用可能な、広範なデータ リポジトリと AI 専門家により実現した Azure 向けの既製の AI サービスを搭載した Microsoft Research の専門知識を活用できます。 

これが、Azure Cognitive Services です。

### <a name="slide-azure-cognitive-services"></a>スライド:Azure Cognitive Services

Azure Cognitive Services には数十を超える API が含まれていますが、より大きなカテゴリで分類すると、以下のような人間が持つ能力にまつわる機能を提供しています。

* 視覚:写真、描画、テキストと手書き、ビデオ内容の認識

* 音声:音声を理解して認識し、自然な人間的な音声を生成するツール。

* 言語:文書やテキストの内容を解釈し、人間の言語間で翻訳します。

* デシジョン: Azure Cognitive Services の最新のカテゴリであり、データ、コンテンツ、アプリケーション ユーザー インターフェイスに関する makinghumanlike の選択肢です。

* 検索: 大規模な非構造化リポジトリのコンテンツに関する自然言語の質問に回答します。 

### <a name="slide-azure-cognitive-services-with-service-names"></a>スライド:Azure Cognitive Services (サービス名を含む)

"検索" カテゴリは、このラーニング パス AIML10 の前の手順で説明しました。 ここでは、その他の利用可能なサービスの一部について説明し、これらのサービスを使用して小売業者の Web サイトを改良します。[クリック]

Computer Vision: 商品の写真の内容を分析するために使用します。

Custom Vision: 小売業者が販売する特定の製品を識別するために使用します。

Personalizer: お客様 (匿名ユーザーを含む) の好みに合わせて Web サイトのレイアウトを自動的に調整し、最初に最適な製品カテゴリを提示します。

ただし、Cognitive Services の設定と使用に関する原則はすべての API で同じであるため、ここで学習する内容は、他の AI サービスでも応用可能です。

### <a name="slide-computer-vision"></a>スライド:Computer Vision

まず、Computer Vision 用にあらかじめ構築された AI を見てみましょう。また、AI を使用してアプリケーションに見る機能を追加し、特定のニーズに合わせてカスタマイズする方法についても説明します。

### <a name="slide-shop-by-photo"></a>スライド:Shop by Photo

これは、ハードウェア小売業者である Tailwind Traders の Web サイトです。 Tailwind Traders の Web サイトには、製品カタログの参照、製品のオンライン注文、小売店で取り扱っている製品の検索機能など、多くの e コマース機能があります。
しかし、これから説明するように、AI 対応の機能もいくつかあります。

ご想像のとおり、Tailwind Traders は架空の会社なので、アプリをご自身でデプロイするために必要なすべてのソース コードを提供することができます。 ソース コードについては、このスライドの下部にあるリンクを参照してください。

### <a name="slide-demo-shop-by-photo"></a>スライド:デモ:Shop by Photo

デモ:"問題の定義:Shop by Photo が壊れています"

Tailwind Traders のライブ Web サイトにアクセスしてみましょう。 [クリック]

AI 対応の機能の 1 つに、"Shop by Photo" (写真でショッピング) と呼ばれるものがあります。 この機能を使用すると、顧客は購入する可能性のある製品の写真をアップロードできます。また、製品が購入可能かどうかをアプリで通知を受け取ることができます。 さあ、試してみましょう。 まず、興味のあるドリルの写真をアップロードします。Tailwind Traders アプリは画像を分析し、それがドリルであることを認識して、Tailwind Traders が販売しているドリルとサイト内のどこにあるかを示します。

では、別の画像で試してみましょう。 ホームページに戻り、"Shop by Photo" 機能をもう一度使用してみましょう。今回は、一組のプライヤーの画像を選びます。 残念ながら、アプリがその画像を分析すると、ハンマーであると認識します。 これは明らかにうまく機能していないので、今度は何が悪かったのかを見つけ、Computer Vision を使用して、修正する方法を見てみましょう。 

### <a name="slide-how-computer-vision-works"></a>スライド:Computer Vision の運用方法

続行する前に、Computer Vision がどのように機能するかを理解すると役に立つでしょう。 計算はほとんどないため、ご心配なく。Computer Vision のしくみについて理解しておくと、問題とその解決方法を理解するのに役立ちます。

### <a name="slide-tasks-xkcd-comic"></a>スライド:タスク (XKCD コミック)

(10 秒間一時停止)

ほんの少し前まで、コンピューターに写真を見せ、画像の内容に関する有益な情報を得ることは、SF の世界でのみ起こりうることでした。 この XKCD は 2014 年 9 月に公開されました。 それから 5 年経った今、コンピューターは、ビッグデータ、GPU コンピューティング、畳み込みニューラル ネットワークの登場により、写真に写っているのが鳥であるかどうかを簡単に判断できます。 その方法を見てみましょう。

### <a name="slide-how-neural-networks-work-brandon-rohrer"></a>スライド:ニューラル ネットワークの仕組み (Brandon Rohrer)

この説明は、AI と機械学習をあらゆる側面から詳しく説明する素晴らしいブログとビデオ チュートリアルを管理している Brandon Rohrer の許可の元参照しています。 今回はその概要のみに触れるため、詳細については、Brandon のブログを参照してください。

### <a name="slide-computer-vision--convolutional-neural-network"></a>スライド:Computer Vision / 畳み込みニューラル ネットワーク

AI が "ディープ ラーニング" と呼ばれるもので動いていることはご存知かと思います。
「ディープラーニング」の「ディープ」は、「深い」という意味ではなく、画像を分析する際に、多くのレイヤーからなるニューラル ネットワークを通過することを意味します。 たったこれだけです。

画面には、非常に単純なニューラル ネットワークが表示されています。 これにはレイヤーが 5 枚しかありませんが、実際に使用されているビジョン システムは、数十から数百のレイヤーからなっています。 これは、画像を入力として受け取り、その画像をちょうど 4 つのオブジェクト (犬、自転車、りんご、テニス ボール) の内 1 つとして分類するように設計されています。 分類できるのはこれだけです。ここで認識するようにトレーニングされているものを除き、他の種類のオブジェクトを検出することはできません。

### <a name="slide-trained-convolutional-nn"></a>スライド:トレーニング済みの畳み込み NN

ニューラル ネットワークをトレーニングすると、ネットワーク レイヤーを 1 枚づつ通して画像を異なる小さな画像に変換しつつ、入力画像が渡されます。 各レイヤーは、前のレイヤーで生成された画像を再結合し、最終的には、0 から 1 の値を持つたった 1 ピクセルになるまで小さくなります。 この値は、ニューラル ネットワークが指定されたオブジェクトを示している自信度を表し、数値が大きいほど、自信があることを意味します。

この例では、自転車の画像を入力し、右側の "bicycle" ノードが最高値を示していることがわかります。 このニューラル ネットワークは、自転車 (少なくともこの特定の自転車) を検出できるよう十分にトレーニングされていることがわかります。 では、どのようにニューラル ネットワークを "トレーニング" し、その過程で画像はどのように変換されるのでしょうか。

ネットワークの各ノード (各円) では、画像にフィルターが適用されます。
これは、スナップショット フィルターや Instagram フィルターのようなものですが、画像をセピア調にしたり、顔にウサギの耳を付ける代わりに、トレーニング プロセスで定義した別の処理を行います。 これについて、詳しく見ていきましょう。

### <a name="slide-filters-1"></a>スライド:フィルター(1)

単純な画像を考えてみましょう。 これは、クロスの画像です。 大きさは 9x9 ピクセル、白は "+1"、黒は "-1" です。 ニューラル ネットワーク内の各ノードで行われているように、この画像にフィルターを適用します。

### <a name="slide-filters-2"></a>スライド:フィルター (2)

この画像を変換するには、3x3 のグリッドの重みを適用します。 このような小さいグリッドは、コンピューターのビジョン システムでよく使用されます (場合によっては 3x3 や 5x5 で、重みはトレーニング プロセスで決定されます)。 このグリッドでは、-1 と 1 の重みだけが使用されますが、通常は、その範囲の乱数のように表示されます。 [クリック]

画像に重みを適用するには、画像の特定のピクセルに中央揃えで重みのグリッドを重ねます。 [クリック] 次に、各ピクセル値ごとに重みを乗算し、平均値を取得します。 この平均値は、重みグリッドの中心となるピクセルに中央揃えされた、出力画像に対応するピクセルになります。

入力画像の端を中心となるピクセルとして使用できないことにお気づきかもしれませんが、これは出力画像は 2 行と 2 列分、入力より小さくなるためです。 これが (他の種類の変換も踏まえて)、レイヤーを移動するにつれて (サイズが 1 ピクセルになるまで) 画像のサイズが小さくなる理由です。

### <a name="slide-filters-3"></a>スライド:フィルター(3)

重みグリッドを下と右にそれぞれ 2 ピクセル移動してみましょう。 重みをソース ピクセルで乗算して平均値を取得すると、異なる出力ピクセル (.55) が取得できます。 ニューラル ネットワークは、ソース 画像の行と列の重みをスイープして、出力画像のピクセルを作成します。

ちなみに、画像全体でフィルターをスイープするプロセスは、シンプルな算術演算でありながら、「畳み込み」という複雑な名前が付けられています。 これが、畳み込みニューラル ネットワークと呼ばれる所以です。

### <a name="slide-training-an-image-classifier"></a>スライド:画像分類器のトレーニング

さて、これでニューラル ネットワーク内の各ノード (円) は、入力画像が変換されたものであり、重みのグリッドによって決定されることがわかりました。 ニューラル ネットワークをトレーニングする秘訣は、正しい数値が最終的に得られるような重みを選択することです。

[クリック] 正しい重みを選択するには、犬、自転車、りんご、テニス ボールの画像が多数含まれているトレーニング データを使用します。 私たちは各画像が表す内容を理解できる (人間が画像見て、ラベル付け、または "注釈" を付けている) ため、各ケースで適切なノードが最大の値を取得できるように、または少なくともできるだけ多くの値を取得できるよう、重みを選択します。

ただし、実際のビジョン ネットワークでは、重みの数は数百万にもおよび、計算対象となるラベル付けされた画像の数は数百万にもなる可能性があります。 重みはどのように決定されますか。

### <a name="slide-learning-backpropagation"></a>スライド:学習:逆伝搬法

機械学習に関するほとんどの書籍は、この時点で数学についての説明を開始、「逆伝搬法」や「学習率」、「コスト関数」といった話を持ち出しはじめます。 しかし、AI の研究員でない限り、次の 2 つの理由から、これらすべてを無視することができます。

まず、ビッグ データ ストアや GPU プロセッサなどの強力なコンピューティング リソースを活用しながら、すべての計算をユーザーの代わりに行う優れたツールが多数用意されています。 Tensorflow や Pytorch のようなツールを耳にしたことがある方もいらっしゃるかと思いますが、このラーニング パスの後半でも説明します。 

次に、とはいえこれらのツールをすべて活用するには、多くのトレーニング データ、強力なコンピューティング リソース、および AI エンジニアのチームが必要です。 代わりに、大量のデータ、コンピューティング、専門知識を既に使用しているプロジェクトまたは会社のリソースを使用してニューラル ネットワークをトレーニングし、それを API 経由で使用することができます。

### <a name="slide-pre-trained-convolutional-nn"></a>スライド:事前トレーニング済みの畳み込み NN

そのため、事前に決められた重みを使用するモデルを使用するだけで済み、モデルでトレーニング済みのオブジェクトの分類を検出するだけの場合は、そのまま利用を開始できます。 画像を提供するだけで、事前トレーニング済みのモデルで生成される分類を使用します。 

モデルによっては、単に分類するだけでなく、画像内のオブジェクトの場所を検出したり、他の方法で画像を分析したりすることもできます。

### <a name="slide-demo-cognitive-services-computer-vision"></a>スライド:デモ:Cognitive Services Computer Vision

次に、事前トレーニング済み AI モデルのCognitive Services Computer Vision を試してみましょう。 このサービスは、指定された画像を分析し、検出したオブジェクトのタグ (または分類) を提供します。 これは、以前の畳み込みニューラル ネットワークの右側にある信頼度が高いスコアに関連付けられるラベルにすぎませんが、ここでは多くのオブジェクトを識別できる Microsoft の強力なニューラル ネットワークを使用しています。

aka.ms/try-computervision でお試しいただける Web ベースの UI をご用意しているため、試してみましょう。 後ほど、プログラムを使用して API にアクセスする方法についても説明します。

### <a name="video-computer-vision-via-web"></a>ビデオ:Web を使用した Computer Vision

[クリック] これは Cognitive Services Computer Vision のページです。 このページで、少し下にスクロールすると、Web ベースのフォームがあります。Ｗeb から、またはローカル ファイルとして、分析用の画像をアップロードできるようになります。 では、ヘルメットを被っている男性の写真をアップロードしてみましょう。 ほんの数秒で、Computer Vision サービスからその画像の分析が返されます。 左側には、画像内の検出されたオブジェクト、右側には、詳細な分析を含む JSON 形式の出力が表示されます。 これには、画像内の検出されたオブジェクトの名前と場所、画像に関連付けられているタグまたはラベルの一覧、画像についての平易な説明 (この場合は "ヘルメットを被っている男性")、およびその他の有用な情報が含まれます。

### <a name="slide-cognitive-services-computer-vision"></a>スライド:Cognitive Services Computer Vision

出力の [オブジェクト] セクションで、帽子の類と、人間という 2 つのオブジェクトが画像内で検出されていることを確認できます。

今回は、画像全体の分類と信頼度スコアを提供する [タグ] セクションに注目します。 この場合、"man" の次に信頼度が高い分類は "headdress" になりますが、これは、Shop by Photo アプリで必要な "hard hat" とは異なります。
残念ながら、この API は、ヘルメットは検出できますが固い帽子を検出できるようトレーニングされていなく、ヘルメットもここでは分類の信頼度が 6 番目となっています。 この問題を解決する方法については、すぐに説明します。

ただし、Web フォームを使用する代わりにアプリにビジョン機能を組み込む場合は、プログラムによって Computer Vision API にアクセスする必要があります。
その方法を見てみましょう。

### <a name="video-computer-vision-via-cli"></a>ビデオ:CLI を使用した Computer Vision

HTTP エンドポイントに接続できる任意の言語を使用して、Cognitive Service API にインターフェイスを設定することができますが、ここでは、[クリック] Azure CLI を使用してリソースを作成し、"curl" を使用して Computer Vision API に接続する bash スクリプトを使用します。 ローカル シェルに Azure CLI をインストールできますが、ここでは Visual Studio Code の "Azure アカウント" 拡張機能を使用して Cloud Shell を起動します。つまり、何もインストールする必要はありません。 シェルの準備ができたら、この bash スクリプトから直接コマンドを実行できます。 

最初のコマンドで、リソース グループを作成します。これは、API を認証するために必要なキーを保持するために使用します。

次の手順で、キーを作成します。 ここでは、Computer Vision を含む多くのサービスで使用できるオムニバスの Cognitive Services キーを作成しています。

これで、キーをターミナルに直接表示できます。 [待機] これらのキーのいずれかを使用して API とやり取りすることができるので、最初のキーを環境変数に保存します。

このキーを使用すると、Computer Vision サービスによって提供されるエンドポイント URL に接続できるため、その URL を環境変数に保存することもできます。

次に、分析する画像を選択できます。 ここでは、画像の URL を指定します。これは、少し前に見たヘルメットを被る男性と同じ画像です。

ここで、curl を使用して JSON 入力を渡すことによって、キーと画像 URL をエンドポイントに渡すことができます。 数ミリ秒で、JSON として画像分析が返されます。 前に Web インターフェイスで見たものと同じ出力が表示されます。

もちろん、好きなイメージを使って行うことができます。 別の画像 (この場合はドリルの画像) でもう一度試してみましょう。 ここでも、curl を使用して API に渡すことができます。 [待機] 興味深いことに、このイメージに関連付けられている最上位のタグは "カメラ" で、残念ながら実際の工具を検索するのには役立ちません。"ドリル" が必要です。

### <a name="slide-adapting-computer-vision-models-with-your-own-data"></a>スライド:Computer Vision モデルを自身のデータに適合させる

これで、Tailwind Traders の Shop by Photo 機能で Computer Vision API が最適な選択肢ではない理由がお分かりいただけたかと思います。 場合によっては、使用するビジョン モデルが Tailwind Traders が販売する特定の製品を特定できるようトレーニングされていない場合もあります。 それ以外の場合は、検出するようにトレーニングされているオブジェクトの種類が*多すぎる*ため、間違ったオブジェクトが検出されています。 先ほど見たように、ドリルの写真を提示したにも関わらず、Tailwind Traders 社が販売していない製品である "カメラ" のタグが返されました。

幸い、この問題は解決することができます。 では、少し理論について説明しましょう。

何千もの画像を識別するために事前トレーニングされている構想モデルから始め、そのオブジェクトが元のモデルのトレーニング データの一部ではない場合でも、目的のオブジェクトだけを識別するように調整できる方法があるとしたら、信じるでしょうか。 奇妙なことのようにも思えますが、どのように動作するかを見てみましょう。

### <a name="slide-transfer-learning"></a>スライド:転移学習

ここでは、前にも使用したトレーニング済みの畳み込みニューラル ネットワークを用意していますが、オブジェクトの分類を行う最後のレイヤーは削除されています。 手元に残るのは、最後から 2 番目のレイヤーから取得した画像です。 画像 (たとえば、3x3 の画像) であることを無視して、これをデータとして捉えます。 信頼スコアを取得する代わりに画像を左側にフィードすると、それぞれが 9 つのデータ ポイントを持つ配列のコレクション、または "特徴" が得られます。 このトイ ネットワークでは、F1、F2 のように F8 までラベル付けされています。
左側に配置する各画像から、右側に異なる特徴のコレクションを生成します。

これらの特徴が*何を*意味しているのかは実際にはわかりませんが、ニューラル ネットワークが*もともと*トレーニングされた画像の種類を分類するのに役立つため、便利であることは確かです。 特徴の 1 つが 「緑っぽさ」を表しており、木やテニスボールを分類するのに役立っているのかもしれません。
または、画像内の円形の領域の数をカウントし、自転車と信号機を分類するのに役立てているのかもしれません。 重要なのは、これらの機能が事前に定義されたわけではないということです。これらはトレーニング データから_学習_したものであり、_一般的な_画像の分類に役立ちます。 

そう、これらの機能を使用して、元のネットワークでトレーニングされていないオブジェクトを分類することができるのです。

### <a name="slide-transfer-learning-training-1---with-the-hammer"></a>スライド:転移学習のトレーニング (1 - ハンマー)

ハンマーと固い帽子を識別する新しいモデルが必要だとします。 左にハンマーの画像を渡し、右側でその特徴を収集します。 この例では、8 つのデータ ベクター (特徴ごとに 1 つ) と、オブジェクト型のバイナリ インジケーターを取得します。 複数のあらゆるハンマーの画像に対してこの手順を繰り返し、データ ベクターとバイナリ インジケーターを毎回収集します。

### <a name="slide-transfer-learning-training-2---with-the-white-hard-hat"></a>スライド:転移学習のトレーニング (2 - 白くて固い帽子)

次に、固い帽子の画像で同じ操作を行います。 今回も、各回ごとに 8 つのデータ ベクターと各画像のバイナリインジケーターを収集します。

すべてをまとめると、手に入るのは それぞれにバイナリの結果が関連付けられたデータ ベクターのコレクションです。 データ サイエンスに触れたことがある場合は、次に何が起こるかお分かりいただけるかと思いますが、ロジスティック回帰や 1 レイヤーからなるニューラル ネットワークなどの単純な予測モデルを構築して、特徴から新しいオブジェクトの分類を予測することができます。

### <a name="slide-transfer-learning-trained-model"></a>スライド:転移学習トレーニング済みモデル

驚くほどうまく機能していることがわかります。 大量のデータが必要なわけではありません。予測するカテゴリがそれぞれ異なっていれば、多くの場合、数十枚の画像で適切に分類できます。 また、比較的少量のデータから数百ほどのバイナリ結果を予測するだけなので、多くのコンピューティング能力は必要ありません。

もちろん、これはトイモデルによる例です。通常は 2 つ以上のオブジェクトを識別する必要があり、最後から 2 番目のレイヤーで基になるニューラル ネットワークによって生成される特徴の数は、8 より多い事は確かです。 しかし、少量の新しいデータとわずかなコンピューティング能力で実現でき、多くの場合、非常にうまく機能するというこの原則はそのままです。

### <a name="slide-microsoft-cognitive-services-custom-vision"></a>スライド:Microsoft Cognitive Services Custom Vision

もちろん、転移学習モデルを自分でトレーニングする必要はありません。 Cognitive Services Computer Vision の高度なビジョン モデルをベースとして使用し、独自の画像と分類を Custom Vision と呼ばれるサービスに提供できます。

Computer Vision と同様に、API を使用してプログラムによる学習モデルのトレーニングを行うこともできますが、Custom Vision は、モデルをトレーニングするための便利な Web UI も提供します。 ここでは、Tailwind Traders の Shop by Photo 機能のモデルをトレーニングするために使用してみましょう。

### <a name="slide-demo-customized-object-recognition"></a>スライド:デモ:カスタマイズされたオブジェクトの認識

デモの手順: https://github.com/microsoft/ignite-learning-paths/blob/master/aiml/aiml20/DEMO%20Custom%20Vision.md

### <a name="video-customvisionai"></a>ビデオ: customvision.ai

[クリック] ここで、Custom Vision Web ベースのインターフェイスについて説明します。 転移学習分析用に新しい画像を提供するのに優れた UI が提供されています。 このプロジェクトには、既に多数の画像がアップロードされています。 ドライバー、ペンチ、ドリル、ハンマーの写真をアップロードしました。これは、カスタムモデルのトレーニング用に使用します。 また、Tailwind Traders が販売しているもう 1 つの製品 (ヘルメット) を検出することもできます。 ここで、[イメージの追加] をクリックし、ハードドライブ上のフォルダーを参照して、ヘルメットの写真をいくつか収集し、それらをすべて選択してサービスに追加し、トレーニングで使用する "hard hat" というラベルを指定します。

これらのファイルをアップロードするには、しばらく時間がかかります。その間は、このプロジェクトに多くの画像が含まれていないことに注意してください: 約 180 または、5 つの各カテゴリに対して数十程度になります。 場合によっては、それより少なくなります。 それにも関わらず、5 つのオブジェクトの種類がかなり異なるため、このモデルは非常にうまく機能します。

では、[トレーニング] ボタンをクリックして、転移学習を開始しましょう。 Quick Training (クイック トレーニング) を選択します。 これで、複雑なビジョンモデルを通じてすべての画像を実行し、転移学習を使用して 5 つのカテゴリの予測モデルを作成しています。 これには数秒しかかかりません。非常に優れたモデルです！
確率のしきい値には、分類がまったくないことを予測する限界を設定します。 信頼度が 50% 以上の分類のみを受け入れる場合、その予測の 90.9% は正しいものになります。これが、"正確度" です。 このモデルでは、画像全体の 88.2 % が正しく分類されます。これが、"リコール" です。 アプリでは、不適切な呼び出しを行う、または呼び出しをまったく行わない許容範囲に応じてしきい値を選択します。 Tailwind Traders の場合、顧客にとって関係性が低い製品を提案することは大きな問題ではないため、低い側にしきい値を設定します。 もしこれが癌を検出するアプリのような場合は、別の呼び出しを行う可能性があります。

次に、まったく見たことのない新しい画像でモデルを試してみましょう。 これを行うには、[クイックテスト] ボタンをクリックします。 "test images" フォルダーから新しいファイルをアップロードします。 "ヘルメットを被っている男性" を試してみましょう。 予測には "ヘルメット" が表示され、確率が 99.9% であることがわかります。そのため、選択したしきい値でその呼び出しを行います。

別の画像、ドリルを試してみましょう。 このモデルでは、画像が 94.5% の確率でドリルとして識別されます。 最後に、ペンチの画像を試してみましょう。これは 99.9% の信頼度で識別されます。

したがって、モデルは 200 枚以下の画像でトレーニングされているにも関わらず、適切に動作します。
これは、潜在的なラベルを Tailwind Traders で販売されている製品のみに制限しているためです。

これで、満足したモデルができましたので、エクスポートしてアプリに組み込むことができます。 [エクスポート] ボタンをクリックすると、iOS または Android のモデルをコンテナーとしてエクスポートしたり、または、この場合はユニバーサルな ONNX 形式でエクスポートしたりできます。 これで、モデルをハード ドライブにダウンロードしました。

### <a name="slide-portable-deep-learning-models"></a>スライド:他のシステムで使えるディープ ラーニング モデル

カスタム モデルを ONNX 形式でエクスポートします。

ONNX (Open Neural Network Exchange) は、無料での AI モデルの交換とデプロイを促進し、幅広いアプリケーションやテクノロジベンダーによってサポートされている、Microsoft と Facebook が立ち上げたオープン スタンダードです。

これで custom vision モデルのトレーニングが完了したので、Tailwind Traders アプリに統合してみましょう。 統合するには、ONNX 形式のモデルから予測を生成する関数を提供するオープンソースの推論エンジンである ONNX Runtime を使用します。

### <a name="slide-onnximagesearchtermpredictorcs"></a>スライド:OnnxImageSearchTermPredictor.cs

カスタム モデルを作成したので、API を使用してアプリで呼び出すことができます。 ここでは、生成した ONNX ファイルから新しい "推論セッション" を作成し、アップロードした画像から文字列として分類ラベルを生成します。
次に、これを Tailwind Traders の Web サイトの既存の検索機能に渡し、結果を表示します。

```csharp 
var session = new InferenceSession(filePath);

...

var output = session.Run(new[] { input });
var prediction = output
    .First(i => i.Name == "classLabel")
    .AsEnumerable<string>()
    .First();
```

### <a name="slide-demo-onnx"></a>スライド:デモ:ONNX

デモ:ONNX のデプロイ

### <a name="video-kudu"></a>ビデオ:Kudu

[クリック] Custom Vision からエクスポートしたモデルは実際には ZIP ファイルであり、実際の ONNX ファイル、先ほど作成したニューラル ネットワークのテキスト表現である model.onnx、マニフェスト ファイルが含まれています。 

既存の Tailwind Traders Web サイトでは、products.onnx と呼ばれる ONNX ファイルのコンピューター ビジョン モデルが既に使用されています。 問題は、Tailwind Traders で販売されている製品の多くがモデルによって正しく認識されないことです。 したがって、Custom Vision からエクスポートしたばかりの、model.onnx ファイルを products.onnx という名前に変更して、Web アプリ内でそのファイルを置き換えます。これにより、トレーニングした 5 つの製品すべてが Shop by Photo によって認識されるようになります。

ここでは、Tailwind Traders Web サイトを実行する App Service リソースを Azure Portal で確認できます。 この App Service でできることは、[開発ツール] セクションにアクセスし、[高度なツール] 機能を選択します。 これにより、Kudu インターフェイスが起動します。 これで、デバッグ コンソールを使用して Web サイトのファイルシステムを参照できます。 products.onnx ファイルが配置されている、OnnxModels、W-ルート、スタンドアロン、サイトを参照してみましょう。 これは、Custom Vision で作成した新しいバージョンの products.onnx ファイルに置き換えることができます。

App Service に戻って、Web サーバーを再起動します。これにより、Shop by Photo 機能で新しい ONNX モデルが使用されるようになります。

### <a name="video-netron"></a>ビデオ:Netron

[クリック] Web サイトが再起動されるのを待っている間に、インストールしたばかりの ONNX モデルを見てみましょう。 Lutz Roeder によって開発された Netron と呼ばれる、ONNX ファイル内のニューラル ネットワークを調べることができるすばらしい Web アプリがあります。 では、products.onnx ファイルを開いてみましょう。 ここでは、モデルによって表されるニューラル ネットワークの実際のレイヤーを確認できます。 少し拡大して、上部にある入力を見てみましょう。 入力は画像です。 これは、サイズが 224x224 ピクセルの 3 枚のレイヤーからなる RGB 画像です。 実際には、ONNX ランタイムに提供する前に、ユーザーから提供された画像をトリミングしてスケールダウンする必要がありました。 これは、コンピューター ビジョン システムの視力はあまりよくなく、つまり非常に低い解像度の画像で動作しているということなのですが、非常にうまく機能しています。

次に、ネットワークで縮小してスクロールします。 Custom Vision で作成されたニューラル ネットワーク内のすべてのレイヤーを見ることができます。このコースの前半で説明したように、各レイヤーは入力画像を変換し、フィルターを適用し、出力画像を再結合します。 しかし、最終的に出力レイヤーに到達すると、出力が 5 つの値のリストであることがわかります -- これは、トレーニングした 5 つの製品 (ハンマー、ヘルメットなど) -- "loss" というラベルの付いたこの値は、モデルが各カテゴリに対して予測する信頼度です。 アプリでは、信頼度の高い要件に合わせて独自のしきい値を選択します。

Tailwind Traders の Web サイトが再起動されたので、ホームページに戻り、新しいビジョン モデルがどのように機能するかを確認しましょう。 それでは、写真をアップロードしてテスト画像の 1 つをもう一度試してみましょう。具体的には、ここまでうまく機能しなかったペンチの画像です。 確かに、これはハンマーであると考えるのではなく、Web サイトは "ペンチ" を検索し、売り出されているすべての製品を表示しています。

### <a name="slide-optimizing-app-ui-with-cognitive-services-personalizer"></a>スライド:Cognitive Services Personalizer によるアプリ UI の最適化

残りの時間を使って、Cognitive Services の "意思決定カテゴリ" から、事前構築された AI の例をもう一つご紹介しようと思います。 この "Personalizer" サービスを使用すると、ユーザーが実行しようとしている可能性が最も高いものと、*提供側がユーザーにやってほしい事*を天秤にかけ、アプリのインターフェイスをリアルタイムでカスタマイズすることができます。

### <a name="slide-recommended-screenshot"></a>スライド:推奨 (スクリーンショット)

これがどのように機能するかについては、Tailwind Traders Web サイトの「おすすめ」セクションで見ることができます。 この図は、ストアで利用できる一連の部門を示しています。1 つは大きな "ヒーロー" の画像で、いくつかの小さな画像と組み合わされています。

Personalizer サービスは、"補強学習" と呼ばれる AI 手法に従って、これらのセクションがどのように表示されるかをユーザーに代わって選択します。

### <a name="slide-personalizer-in-action"></a>スライド:Personalizer が作動中

Microsoft では、長年にわたって Personalizer の開発を行ってきました。 これは、Xbox デバイスで使用されており、インストールされているゲームをプレイする、新しいゲームをストアから購入する、Mixer を通じて他のユーザーのゲームプレイを視聴するなど、ホーム ページでおすすめするアクティビティを決定しています。 Personalizer が導入されたことにより、Xbox チームでは主要なエンゲージメント メトリックの上昇を目の当たりにしています。

Personalizer はまた、Bing 検索の広告の配置の最適化や、MSN ニュースで取り上げられる記事にも使用され、ユーザー エンゲージメントの向上において、優れた結果を残しています。

この度、独自のアプリでも Personalizer が使用できるようになりました。

### <a name="slide-reinforcement-learning"></a>スライド:強化学習

Personalizer は強化学習と呼ばれる AI 手法を実装しています。 しくみは次のとおりです。

[クリック] ユーザーに "ヒーロー" アクションを表示するとします。 [クリック] ユーザーは次の操作を行うことができない場合がありますが、[クリック] いくつかの提案を表示できます。 ゲームアプリの場合は、[クリック] "ゲームの再生"、"映画を見る"、または "クランに参加" などでしょう。 [クリック] そのユーザーの履歴やその他のコンテキスト情報 (たとえば、場所、時刻、曜日など) に基づいて、Personalizer サービスは、[クリック] 考えられるアクションをランク付けし、[クリック] 促進すべきものを提案します[クリック]。 

恐らく、ユーザーは満足するでしょう。[クリック] しかし、どうすればよいでしょうか。 これは、ユーザーが次に行うこと、および、それがやりたかったことなのかどうかによって異なります。
ビジネス ロジックに従って、[クリック] 次のように、0 ～ 1 の "報酬スコア" を割り当てます。 たとえば、ゲームをもっとプレイしたり、記事を読んだり、ストアでより多くの金額を課金したりすると、より高い報酬スコアにつながる可能性があります。 [クリック] Personalizer は、次にアクティビティを特徴付ける必要がある時のために、このような情報をランク付けシステムに提供します。

### <a name="slide-discovering-patterns-and-causality"></a>スライド:パターンと因果関係の探索

しかし、これはユーザーに既に知られているものを提示する危険性がある、ただのレコメンダー システムではありません。 ユーザーが求めているが、求めていることを認識していないものは何でしょうか。 通常、Personalizer は履歴とコンテキストに基づいて最適なアクティビティを推奨するエクスプロイト モードですが、時に探索モードに入り、通常では目の当たりにしないような新しいものをユーザーに提示します。 これは自動化された A/B テスト システムに似ていますが、3 つ以上の分岐があり、すべてリアル タイムでテストされます。

探索モードがアクティブ化される割合を制御し、ユーザーが新しいコンテンツまたは機能を検出できるようにします。

### <a name="slide-personalizer-for-tailwind-traders"></a>スライド:Tailwind Traders 用の Personalizer

Tailwind Traders アプリでは、匿名ユーザー向けに、時間、曜日、ブラウザーの OS を "コンテキスト" として使用して順位付けに影響を及ぼします。 報酬スコアは、ヒーローのセクションがクリックされたかどうかに関わらず使用します。 このコードでは、おすすめカテゴリがクリックされた場合は 1、それ以外の場合は 0 の報酬スコアを提供します。

時間の経過に応じて、Personalizer は時間、曜日、OS に基づいて匿名ユーザーに最適なカテゴリを決定します。 また、時間の 20% を "探索" の時間に充て、埋もれているカテゴリーを発掘します。

### <a name="slide-demo-personalizer"></a>スライド:デモ:Personalizer

[クリック] 次に、動作中の Personalizer を見てみましょう。 Tailwind Traders ホーム ページに戻ってみましょう。 これまで説明していなかったことは、この候補セクションでは、製品部門の順序が Personalizer により決まるということです。
この例では、電気部門がヒーローの画像として表示されています。 Web サイトを何度も更新すると、この "探索" 動作を確認することができます。
明らかに、Personalizer は使用しているブラウザーとオペレーティング システムを使用して、この時点では、Garden Center は匿名のユーザーから最適なエンゲージメントを得ていると考えていますが、最終的にはさまざまなカテゴリを試しています。ポップアップが表示され、それを使用してエンゲージメントも測定します。

### <a name="slide-pre-built-ai-in-production"></a>スライド:実稼働環境での事前構築された AI

事前構築済み AI を使用して、アプリケーションを人間に似た機能を使用して拡張するいくつかの方法を見てきました。 最後に、これらのアプリケーションを実稼働アプリ、例えば百万ものユーザー向けのリアルタイムな機能を含むアプリに展開する予定がある場合は、いくつかの点に注意してください。

### <a name="slide-cost-considerations"></a>スライド:コストに関する考慮事項

まず最初に考えるべきは、最終的にかかるコストかと思います。

[クリック] "開発者のように" あらゆるものを試す場合、データ量はごくわずかであり、いくつかのことを試すだけであれば、通常は無料です。 

[クリック] 実稼働のボリュームの場合、使用しているサービスに応じて、ボリュームと料金に応じて課金されます。

[クリック] 料金の詳細については、こちらをのリンクを参照してください。 サービスとリージョンごとの正確な価格について確認してください

Azure を初めて使用する方で、これらのサービスを試したい場合は、こちらのリンクを使用してサインアップすると $200 を入手できます。

(このスライドは、cognitive services の価格の "モデル" となる一般的な概要を提供することを目的としています。 参加者は、指定されたリンクから、使用するサービスの正確な価格を確認する必要があります。)

### <a name="slide-data-considerations"></a>スライド:データに関する考慮事項

また、データの送信先とその使用方法についても考慮する必要があります。

画像やテキストなどのデータは、推論を行うために Azure にアップロードされますが、Cognitive Services に保存されることはありません。 このリンクから、プライバシーと規制遵守に関するすべての詳細情報をご確認いただけます。 しかし、ファイアウォールを超えてデータを転送することができない医療業界に従事している場合は、コンテナーという選択肢もあります。

### <a name="slide-deployment-with-containers"></a>スライド:コンテナーを使用したデプロイ

一部の Cognitive Services は、独立したコンテナーとして使用できます。 コンテナー画像をダウンロードし、ファイアウォールの内側にデプロイして、Azure と同じように提供されるローカル エンドポイントを使用するだけです。 違いは、データが独自のネットワークから外に送信されることはないということです。 コンテナーが Azure に接続する唯一の場合が、課金のためです。使用量は、Azure 自体とまったく同じ方法で課金されます。

### <a name="slide-ethical-considerations"></a>スライド:倫理的な考慮事項

最後に、最も重要なスライドを残してあります。 ここまで、強力な AI 機能をアプリケーションに簡単に統合できることを見てきました。 しかし、大いなる力には大いなる責任が伴うように、アプリケーションがユーザーに与える影響を理解し、倫理的な影響を考慮することが非常に重要です。

AI テクノロジを使用している場合は、次のような倫理的なフレームワーク内で作業する必要があります。

* 人間を AI に置き換えるのではなく、ユーザーが既に行っている内容をより多く*達成できるように*することに焦点を当てる

* すべての種類のユーザーを*含め*、すべてのユーザーがアプリケーションから恩恵を受けることができる

* 公正かつ透明性があり、特にマイノリティのグループを阻害しないこと。 前に学んだことを思い出してください。AI は、トレーニングされたデータ以上の成果を発揮することはできません。また、アプリケーションは、ユーザーの身分や見た目に関係なく、すべてのユーザーに対して動作することを確認する必要があります。

倫理的なフレームワークをお持ちでない場合は、こちらのリンクから確認できる、人工知能に関する Microsoft 独自の原則から開始することをお勧めします。

### <a name="slide-wrapping-up"></a>スライド:まとめ

事前構築された AI を使えば、人間に似た機能を簡単に追加できます。 事前に構築されたモデルですべてを行うことはできませんが、手早く結果を出すことができます。 このラーニング パスでは、残りの 20% のカスタム モデルについて説明します。

AI は強力ですが、魔法ではありません。 AI は非常に単純な数学を中心に据え、データによって駆動されます。 データを常に考慮し、何が起こっているかを理解するために使用します。 特に、最高峰の AI でも、トレーニング データで適切に表現されていないグループについては、間違う可能性があることに注意してください。

最後に試してみましょう。 使用を開始するために多くの専門知識は必要ありませんが、すべてのユーザーが AI の倫理的な影響とユーザーへの影響について理解している必要があります。そのため、AI を使用するための倫理的なフレームワークを開発し、それに従うようにしてください。

### <a name="slide-docs-alert"></a>スライド:Docs のお知らせ

ファースト ステップ ガイドとリファレンスなど、Azure Cognitive Services の詳細については、Microsoft Docs を参照してください。

### <a name="slide-ms-learn-alert"></a>スライド:MS Learn のお知らせ

Cognitive Services の使用方法について学習したい場合は、無料のコースが Microsoft Learn にあり、ステップバイステップで学ぶことができます。

### <a name="slide-resources"></a>スライド:リソース

Docs and Learn へのリンクと、このプレゼンテーションで説明したすべてのリソースについては、このスライドのセッション リソースのリンクをご覧ください。 また、この github リポジトリで提供されているコードとスクリプトを使用して、本日お見せしたデモをご自身で実行することもできます。 AI またはデータ サイエンスで Microsoft の認定を受けたい場合は、本日の参加者の皆様向けの無料の証明書付与の特別プランをご利用ください。詳細については、こちらのリンクをご覧ください。

ありがとうございました。
