# <a name="aiml20-speaker-notes"></a>AIML20:講演者用メモ

スクリプトにしたテキストではなく箇条書きを使用したい場合、この PPT プレゼンテーションの各スライドの要点はこちらで確認できます: https://microsoft.sharepoint.com/:p:/t/CloudDevAdvocacy/EctuTXQCOdpGqc5lhQgUnMgBr-R6hlWR5MuLE3qCIFgoHA?e=r1szwF

関連するデモ スクリプトについては、 https://github.com/microsoft/ignite-learning-paths/tree/master/aiml/aiml20を参照してください。 `DEMO%20Setup.md`で開始します。

## <a name="slide-notes"></a>スライド メモ

スライドはタイトルでのみ識別されます。

### <a name="slide-microsoft-ignite-the-tour"></a>スライド:Microsoft Ignite the Tour

プレゼンテーション前の導入スライド

### <a name="slide-using-pre-built-ai-to-solve-business-problems"></a>スライド:事前構築済み AI を使用してビジネスの課題を解決する

自己紹介します。

コンピューティング リソースや AI の専門知識がない場合でも、AI 機能をアプリケーションに追加する方法。

クラウドで事前構築された AI サービスを使用する。

### <a name="slide-resources"></a>スライド:リソース

豊富なリンク、リソース、デモを用意しています。

アプリの完全なソース コードやワンクリック デプロイを含む、すべてのものへのリンクがこちらにあります。

このスライドは、最後に再び表示されます。

### <a name="slide-adding-humanlike-capabilities-to-apps"></a>スライド:人間的な機能をアプリに追加

ここでは、事前構築された AI サービスを使用してアプリケーションに人間的な機能を追加しますが、これはどうことなのでしょうか？ 次に例をいくつか示します。

### <a name="slide-enhance-apps-with-humanlike-capabilities"></a>スライド:人間的な機能でアプリを強化

[クリック] アプリに音声機能 (チャットなど) を追加します。
 
[クリック] アプリに視覚機能を追加します (画像を理解する)。

[クリック] アプリに人間の行動に関する直感を与えます (インターフェイスを適応させる)。

[クリック] アプリに理解力を与えます (あらゆる言語でコミュニケーションする)

[クリック] 人間がデータ ストリームをスキャンして異常を検出する (およびスケーリングする) プロセスを自動化します

いくつかの例を示します。

### <a name="slide-overview-of-azure-cognitive-services"></a>スライド:Azure Cognitive Services の概要

人間的な AI を実装するには、大量のデータと技術的な専門知識が必要ではありませんか。

答えは、いいえです。 Microsoft Research の専門知識を活用してください。 単純な REST API 呼び出しを使用して機能を追加します。 

これが、Azure Cognitive Services です。

### <a name="slide-azure-cognitive-services"></a>スライド:Azure Cognitive Services

Azure Cognitive Services には、20 を超える数の API が用意されています。

人間的能力のカテゴリに含まれるサービス:

視覚:写真、描画、テキストや手書き、ビデオの内容を理解

音声:音声を理解して認識し、自然で人間的な音声を生成するツール。

言語:文書やテキストの内容を解釈し、人間の言語間で翻訳します。

デシジョン: Azure Cognitive Services の最新のカテゴリで、データ、コンテンツ、アプリケーション ユーザー インターフェイスについて人間のような選択を行うことを目的としています。

検索: 大規模な非構造化リポジトリのコンテンツに関する自然言語の質問に回答します。 

### <a name="slide-azure-cognitive-services-with-service-names"></a>スライド:Azure Cognitive Services (サービス名を含む)

"検索" は既にカバーされています。 

利用可能なサービスを他にもいくつか使用して、小売用 Web サイトを強化します。 [クリック]

Computer Vision: 製品の写真の内容を分析するために使用します。

Custom Vision: 小売業者が販売する製品を特定するために使用します。

Personalizer: Web サイトのレイアウトを自動的に適用します。 

ただし、Cognitive Services の設定と使用の原則はすべての API に共通しているため、ここで学習する内容はどの AI サービスを使用したい場合でも応用できます。

### <a name="slide-computer-vision"></a>スライド:Computer Vision

まず、Computer Vision 用に事前構築された AI を見てみましょう。

### <a name="slide-shop-by-photo"></a>スライド:Shop By Photo

これは、ハードウェアの小売販売業者 (架空の会社)  Tailwind Traders の Web サイトです。 

### <a name="slide-demo-shop-by-photo"></a>スライド:デモ:Shop By Photo

デモ:"問題の定義:Shop by Photo が壊れています"

### <a name="video-shop-by-photo"></a>ビデオ:Shop by Photo

Tailwind Traders のライブ Web サイトにアクセスしてみましょう。 [クリック]

AI 対応の機能の 1 つに、"Shop by Photo" と呼ばれるものがあります。 この機能を使用すると、顧客は購入したい製品の写真をアップロードできます。そして、その製品が購入可能かどうかをアプリに教えてもらうことができます。 さあ、試してみましょう。 まず、自分が興味のあるドリルの画像をアップロードします。すると Tailwind Traders アプリではその画像を分析してドリルであることを認識し、Tailwind Traders が販売しているドリルを表示して、それがストア内のどこにあるかを示します。

では、別の画像で試してみましょう。 ホームページに戻り、"Shop by Photo" 機能をもう一度使用します。今回は、ペンチの画像を選びます。 残念ながら、アプリでその画像を分析すると、ハンマーであると認識します。 これは明らかにうまく機能していません。それでは次に、何がよくなかったのかを分析し、Computer Vision を使用して修正することができるかどうか見てみましょう。 

### <a name="slide-how-computer-vision-works"></a>スライド:Computer Vision のしくみ

ここでは、少し理論的に説明します。 数学的ではありません。

問題の原因と解決方法を理解するのに役立ちます。

### <a name="slide-tasks-xkcd-comic"></a>スライド:タスク (XKCD コミック)

(10 秒間一時停止)

写真を理解するコンピューターというのは、かつては文字通りサイエンス フィクションにすぎませんでした。

5 年後の現在、これは可能であるだけではなく、簡単です。

### <a name="slide-how-neural-networks-work-brandon-rohrer"></a>スライド:ニューラル ネットワークのしくみ (Brandon Rohrer)

Brandon Rohrer 氏から許可を得て改変しています。

彼のビデオ チュートリアル シリーズのブログをご覧ください。AI と機械学習に関する多くの特徴について詳しく説明しています。 

### <a name="slide-computer-vision--convolutional-neural-network"></a>スライド:Computer Vision / 畳み込みニューラル ネットワーク

AI は "ディープ ラーニング" によって機能しますが、"ディープ" は " 深い" を意味するものではありません。

これは単純な NN です。 実際にはもっと多くのレイヤーがあります。

これは 5 つのオブジェクトだけを検出するように設計されています。 それ以外のものを認識することはできません。

### <a name="slide-trained-convolutional-nn"></a>スライド:トレーニングされた畳み込み NN

画像は左側から入力されます。 

各ノードで元の画像を処理して再結合し、最後に単一の値になるまで圧縮します。これが分類の信頼度です。

ここでは、入力が自転車で、右側のノードの値が "bicycle" になっています。 画像が正しく識別されました。

### <a name="slide-filters-1"></a>スライド:フィルター (1)

時間の都合で省略されています。

### <a name="slide-filters-2"></a>スライド:フィルター (2)

時間の都合で省略されています。

### <a name="slide-filters-3"></a>スライド:フィルター (3)

時間の都合で省略されています。

### <a name="slide-training-an-image-classifier"></a>スライド:画像分類器のトレーニング

では、これを行うために NN をどのようにトレーニングすればよいでしょうか。

[クリック] 正しいフィルターを選択することです。 各フィルターは、重みの小さいグリッド (通常は 3 x 3 または 5 x 5) によって制御されます。

[クリック] 分類がわかっている多くのトレーニング画像を使用して重みを選択します。 正しい分類が選択されるように (または少なくともほとんどの場合にそうなるように) 重みを選択します。

実際の視覚ネットワークでは、重みの選択肢は数百万にもおよびます。 重みはどのように決定しますか。

### <a name="slide-learning-backpropagation"></a>スライド:学習:逆伝搬法

ここで、難しい数学が登場します。

ですが、AI エンジニアにでもならない限り、気にする必要はありません。 他者によって重みが最適化されたネットワークを使用するだけです。

これは、アプリケーションの約 80% をカバーしています。 ただし、独自の NN を設計して重みを最適化する必要がある場合は、Tensorflow や PyTorch のようなツールがあります。 AIML40 と AIML50 の内容について説明します。

### <a name="slide-pre-trained-convolutional-nn"></a>スライド:事前にトレーニングされた畳み込み NN

しかし、必要な画像を検出できる NN にアクセスできる限り、あなたは画像を提供するだけで十分です。このネットワークでその分類を行います。

モデルによっては、単に分類するだけでなく、オブジェクトの場所を検出したり、他の方法で画像を分析したりすることができます。

### <a name="slide-demo-cognitive-services-computer-vision"></a>スライド:デモ:Cognitive Services Computer Vision

Cognitive Services Computer Vision には、数千ものオブジェクトを分類できる強力な NN が用意されています。

aka.ms/try-computervision で、シンプルな Web ベースの UI を使用して試すことができます

今すぐ試してみましょう。

### <a name="video-computer-vision-via-web"></a>ビデオ:Web を使用した Computer Vision

[クリック] これは Cognitive Services Computer Vision のページです。 このページで、少し下にスクロールすると、Web ベースのフォームがあります。これを使用して Ｗeb から、またはローカル ファイルとして保存されている分析用の画像をアップロードできます。 では、安全帽を被っている男性の写真をアップロードしてみましょう。 ほんの数秒で、Computer Vision サービスからその画像の分析が返されます。 左側には画像内で検出されたオブジェクト、右側には詳細な分析を含む JSON 形式の出力が表示されます。 これには、画像内で検出されたオブジェクトの名前と場所、画像に関連付けられているタグまたはラベルの一覧、画像についての平易な説明 (この場合は "ヘルメットを被っている男性")、およびその他の有用な情報が含まれます。

### <a name="slide-cognitive-services-computer-vision"></a>スライド:Cognitive Services Computer Vision

[オブジェクト] を見てみましょう。2 つのオブジェクトが検出されました。 帽子の類と、人間です。

[タグ] を確認します。 信頼度が最も高いのは man です。 次は Headdress です。 Helmet は 6 番目でしかありません。
モデルは、"安全帽" については特にトレーニングされていません。

この問題を解決する方法については、後程説明します。

アプリに視覚を組み込む場合は、プログラムを使用して API にアクセスできます。 その方法を見てみましょう。

### <a name="video-computer-vision-via-cli"></a>ビデオ:CLI を使用した Computer Vision

HTTP エンドポイントに接続できるものであれば、どの言語を使用しても Cognitive Service API へのインターフェイスを設定することができますが、ここでは、[クリック] Azure CLI を使用してリソースを作成し、"curl" を使用して Computer Vision API に接続する bash スクリプトを使用します。 ローカル シェルに Azure CLI をインストールすることもできますが、ここでは Visual Studio Code の "Azure Account" 拡張機能を使用して Cloud Shell を起動します。つまり、何もインストールする必要はありません。 シェルの準備ができたら、この bash スクリプトから直接コマンドを実行できます。 

最初のコマンドで、リソース グループを作成します。これは、API を認証するために必要なキーを保持するために使用します。

次の手順で、キーを作成します。 ここでは、Computer Vision を含む多くのサービスで使用できるオムニバスの Cognitive Services キーを作成しています。

それから、このキーをターミナルに直接表示できます。 [待機] これらのキーのどちらを使用しても API へのインターフェイスを設定することができます。ここでは、最初のものを環境変数に保存します。

このキーを使用すると、Computer Vision サービスによって提供されるエンドポイント URL に接続できます。その URL も同様に環境変数に保存しましょう。

次に、分析する画像を選択できます。 ここでは、画像の URL を指定します。しばらく前に見たものと同じ、安全帽を被った男性の画像です。

ここで、curl を使用して JSON 入力を渡すことによって、キーと画像の URL をエンドポイントに渡すことができます。 数ミリ秒で、JSON として画像分析が返されます。 前に Web インターフェイスで見たものと同じ出力が表示されます。

もちろん、どれでも好きな画像を使って行うことができます。 別の画像 (この場合はドリルの画像) でもう一度試してみましょう。 ここでも、curl を使用してこれを API に渡すことができます。 [待機] 興味深いことに、この画像に関連付けられている最上位のタグは "camera" で、残念ながら実際の工具を検索するのには役立ちません。"drill" が必要です。

### <a name="slide-adapting-computer-vision-models-with-your-own-data"></a>スライド:Computer Vision モデルを自身のデータに適合させる

Computer Vision API は、Shop by Photo では機能しません。 

あまりに多くのオブジェクトの種類を検出するようにトレーニングされています。

幸い、この問題を解決することができます。 では、少し理論について説明しましょう。

モデルを何千ものオブジェクトに適合させ、目的のオブジェクトだけを検出するように調整する方法があります。 それが元のモデルに含まれていなかった場合でさえ。 

転移学習と呼ばれる AI 手法を使用して、その方法を見てみましょう。 

### <a name="slide-transfer-learning"></a>スライド:転移学習

前と同じ CNN ですが、最後のレイヤーが削除されています。

最後から 2 番目のレイヤーでは "特徴" が提供されます。これは数値のベクターと考えることができます。 各画像で異なる特徴のセットが生成されます。

その特徴が何を表すのかはわかりませんが、一般的に画像を分類するのに役立ちます。

トリックは、これらの特徴を使用すれば新しいオブジェクトのセットを分類できるということです。

### <a name="slide-transfer-learning-training-1---with-the-hammer"></a>スライド:転移学習のトレーニング (1 - ハンマー)

転移学習を使用して、ハンマーと安全帽を識別するモデルを作成します。

ハンマーの画像を渡し、特徴を収集します。 さらに "hammer" のバイナリバイナリ標識を収集します。 多くのハンマーに対して繰り返します。

### <a name="slide-transfer-learning-training-2---with-the-white-hard-hat"></a>スライド:Transfer Learning のトレーニング (2 - 白いヘルメット)

ここで、ヘルメットの画像を使用して同じ操作を行います。

いずれの場合も、8 つのデータ ベクターと各イメージのバイナリ インジケーターを収集します。

すべてをまとめると、それぞれにバイナリの結果が関連付けられたデータ ベクトルのコレクションができあがります。 

これにより、簡単な予測モデルを作成できます。

### <a name="slide-transfer-learning-trained-model"></a>スライド:転移学習でトレーニングされたモデル

これは驚くほどうまく機能します。

大量の画像やコンピューティング能力は必要ありません。

これはトイモデルによる例ですが、この原則は大規模なモデルにも当てはまります。

### <a name="slide-microsoft-cognitive-services-custom-vision"></a>スライド:Microsoft Cognitive Services Custom Vision

転移学習モデルを自分でトレーニングする必要はありません。

トレーニングされた Microsoft の視覚モデルの 1 つを使用して、Custom Vision で独自のオブジェクトの画像に適応させます。

Shop by Photo 用の視覚モデルを作成してみましょう。

### <a name="slide-demo-customized-object-recognition"></a>スライド:デモ:カスタマイズされたオブジェクト認識

デモの手順: https://github.com/microsoft/ignite-learning-paths/blob/master/aiml/aiml20/DEMO%20Custom%20Vision.md

### <a name="video-customvisionai"></a>ビデオ: customvision.ai

[クリック] これが、Custom Vision の Web ベースのインターフェイスです。 優れた UI を使用して、転移学習分析用の新しい画像を提供できます。 ご存知のとおり、このプロジェクトには既に多数の画像がアップロードされています。 ドライバー、ペンチ、ドリル、ハンマーの写真をアップロードしました。これらをカスタム モデルのトレーニングに使用します。 さらに、Tailwind Traders が販売している製品をもう 1 つ検出したいと思います。安全帽です。 ここで、[Add images]\(画像の追加\) をクリックし、ハード ドライブ上のフォルダーを参照します。ここには安全帽の写真が何枚か収集されています。それらをすべて選択してサービスに追加し、トレーニングで使用する "hard hat" というラベルを指定します。

これらのファイルをアップロードするには、しばらく時間がかかります。その間にご覧いただきたいのは、このプロジェクトにそれほど多くの画像が含まれていないことです: 約 180 枚、または 5 つのカテゴリそれぞれに対して 2, 3 ダース程度です。 場合によっては、もっと少数です。 それでも、この 5 つのオブジェクトの種類がかなり異なるため、このモデルは非常にうまく機能します。

では、[Train]\(トレーニング\) ボタンをクリックして、転移学習を開始しましょう。 [Quick Training]\(クイック トレーニング\) を選択します。 現在、複雑な視覚モデルに対してこれらすべての画像を処理し、転移学習を使用してこの 5 つのカテゴリに対する予測モデルを作成しています。 数秒しかかかりません。非常に優れたモデルです。
確率のしきい値には限界が設定されています。それを下回ると、分類をまったく予測しません。 信頼度が 50% 以上の分類のみを受け入れる場合、その予測の 90.9% は正しいものになります。これが "精度" です。 そして、このモデルでは画像全体の 88.2% が正しく分類されます。これは "再現率" です。 アプリでは、誤った判断を下すことを許容するか、まったく判断しないかを考慮してしきい値を選択します。 Tailwind Traders の場合、顧客に誤った製品を提案してもたいした問題ではないので、低い方にしきい値を設定できます。 もしこれががんを検出するアプリであれば、おそらく別の判断をするでしょう。

次に、まったく処理したことのない新しい画像でこのモデルを試してみましょう。 これを行うには、[Quick Test]\(クイックテスト] ボタンをクリックします。 "test images" フォルダーから新しいファイルをアップロードします。 "安全帽を被っている男性" を試してみましょう。 すると、その予測がまさに "hard hat" であり、確率が 99.9% であることがわかります。おそらく、どのようなしきい値を選択しても、同様の判断を下すことができるでしょう。

別の画像 (ドリル) を試してみましょう。 このモデルでは、この画像が確率 94.5% のドリルとして識別されます。 最後に、ペンチの画像を試してみましょう。これは 99.9% の信頼度で識別されます。

したがって、このモデルは 200 枚以下の画像でトレーニングされたにも関わらず適切に動作しています。
これは、潜在的なラベルを Tailwind Traders で販売されている製品のみに制限したためです。

これで満足のいくモデルができましたので、エクスポートしてアプリに組み込むことができます。 [Export]\(エクスポート\) ボタンをクリックすると、iOS または Android 用のモデルをコンテナーとしてエクスポートしたり、この場合のようにユニバーサルな ONNX 形式でエクスポートしたりできます。 これで、このモデルをハード ドライブにダウンロードしました。

### <a name="slide-portable-deep-learning-models"></a>スライド:移植可能なディープ ラーニング モデル

カスタム モデルを ONNX 形式でエクスポートしました。

ONNX、または Open Neural Network Exchange は、AI モデルの無料交換やデプロイを促進するために Microsoft と Facebook が立ち上げたオープンな標準フォーマットであり、幅広いアプリケーションやテクノロジ ベンダーにサポートされています。

ONNX Runtime を使用して、エクスポートされたモデルを Web サイトに統合しました。

### <a name="slide-onnximagesearchtermpredictorcs"></a>スライド:ONNXImageSearchTermPredictor.cs

InferenceSession は、エクスポートされた .onnx ファイルを参照します。

モデルでは分類ラベルが生成され、検索に渡されます。

### <a name="slide-demo-onnx"></a>スライド:デモ:ONNX

デモ:ONNX のデプロイ

### <a name="video-kudu"></a>ビデオ:Kudu

[クリック] Custom Vision から先ほどエクスポートしたモデルは実際には ZIP ファイルです。これには、実際の ONNX ファイルであり先ほど作成したニューラル ネットワークのテキスト表現である model.onnx と、マニフェスト ファイルが含まれています。 

既存の Tailwind Traders Web サイトでは、products.onnx と呼ばれる ONNX ファイルで表現されるコンピューター ビジョン モデルが既に使用されています。 問題は、Tailwind Traders で販売されている製品の多くがこのモデルによって正しく認識されないことです。 したがって、Custom Vision からエクスポートしたばかりの model.onnx ファイルを products.onnx という名前に変更して、この Web アプリ内のそのファイルを置き換えます。これにより、トレーニングした 5 つの製品すべてが Shop by Photo によって認識されるようになります。

この Azure portal では、Tailwind Traders Web サイトを実行する App Service リソースを確認できます。 この App Service 内で、[Development Tools]\(開発ツール\) セクションの、[Advanced Tools]\(高度なツール\) 機能を選択します。 これにより、Kudu インターフェイスが起動します。 起動したら、デバッグ コンソールを使用してこの Web サイトのファイルシステムを参照できます。 [site]、[w-root]、[Standalone]、[OnnxModels] を参照します。ここに products.onnx ファイルが配置されています。 ここで、Custom Vision で作成した新しいバージョンの products.onnx ファイルに置き換えることができます。

App Service に戻って、Web サーバーを再起動します。これにより、Shop by Photo 機能で新しい ONNX モデルが使用されるようになります。

### <a name="video-netron"></a>ビデオ:Netron

[クリック]  Web サイトが再起動されるのを待っている間に、インストールしたばかりの ONNX モデルの内容を見てみましょう。 Lutz Roeder 氏によって開発された、Netron と呼ばれる小さなすばらしい Web アプリがあります。これを使って、ONNX ファイル内のニューラル ネットワークを調べることができます。 では、その products.onnx ファイルを開いてみましょう。 ここでは、このモデルによって表されるニューラル ネットワークの実際のレイヤーを確認できます。 少し拡大して、上部にある入力を見てみましょう。 入力は画像です。 これは、サイズが 224 x 224 ピクセルの 3 枚のレイヤーからなる RGB 画像です。 実際には、ONNX runtime に提供する前に、ユーザーが提供した画像をトリミングしてスケールダウンする必要がありました。 これは、コンピューター ビジョン システムの視力があまりよくない、つまり解像度がかなり低い画像で動作しているにもかかわらず、非常にうまく機能しているということです。

次に、縮小してネットワークをスクロールしてみましょう。 Custom Vision で作成されたニューラル ネットワーク内のすべてのレイヤーを見ることができます。この講義の前半で説明したように、各レイヤーは入力画像を変換し、フィルターを適用し、出力イメージを再結合します。 しかし、最終的に出力レイヤーに到達すると、出力が 5 つの値のリストであることがわかります。これは、トレーニングした 5 つの製品 (ハンマー、安全帽など) を示します。このモデルが各カテゴリに対して予測する信頼度を示す "loss" というラベルの付いた値もあります。 アプリでは、信頼度の高さについての要件に合わせて独自のしきい値を選択します。

Tailwind Traders Web サイトが再起動されたので、ホームページに戻り、新しい視覚モデルがどのように機能するかを確認しましょう。 それでは、写真をアップロードして、テスト画像の 1 つをもう一度試してみましょう。具体的には、これまでにうまく機能しなかったペンチの画像です。 Web サイトではこれがハンマーであるとは考えず、確かに "ペンチ" を検索し、売り出されているすべての製品を表示しているのがわかります。

### <a name="slide-optimizing-app-ui-with-cognitive-services-personalizer"></a>スライド:Cognitive Services Personalizer によるアプリ UI の最適化

もう 1 つの簡単な例を次に示します。Personalizer。

"Personalizer" サービスを使用すると、ユーザーの動作から学習して、リアルタイムでアプリのインターフェイスをカスタマイズできるようになります。

### <a name="slide-recommended-screenshot"></a>スライド:おすすめ (スクリーンショット)

[おすすめ] セクションでは、1 つの大きな "ヒーロー" 画像をいくつかの小さな画像と組み合わせて表示します。

Personalizer ではセクションが現れる順番を選択します。

"強化学習" と呼ばれる AI 手法を使用します。

### <a name="slide-personalizer-in-action"></a>スライド:Personalizer 稼働中

Personalizer は、Microsoft で長年にわたって開発されてきました。 

XBox、Bing および MSN ニュースで使用されています。

今では、自分のアプリでも Personalizer を使用できるようになりました。

### <a name="slide-reinforcement-learning"></a>スライド:強化学習

Personalizer は強化学習と呼ばれる AI 手法を実装しています。 しくみは次のとおりです。

[クリック] ユーザーに "ヒーロー" アクションを表示するとします。 [クリック] ユーザーは次に何をするか決めていないかもしれません [クリック] が、いくつかの候補のうちのひとつを私たちが提案できるかもしれません。 ゲームアプリの場合は、[クリック] "ゲームの再生"、"映画を見る"、または "クランに参加" などでしょう。 [クリック] そのユーザーの履歴やその他のコンテキスト情報 (たとえば、場所、時刻、曜日など) に基づいて、Personalizer サービスでは、[クリック] 考えられるアクションをランク付けし、[クリック] 最も良いものを提案します。 

ユーザーが満足することが望ましいですが [クリック] 、どうすればそれが確実にわかるでしょうか。 それは、このユーザーが次に行うことと、それが私たちの提案と一致したかどうかによって判断できます。
ビジネス ロジックに従って、[クリック] 次に起こることに対して 0 から 1 の "報酬スコア" を割り当てます。 たとえば、ゲームをもっと長くプレイしたり、記事を読んだり、ストアでもっとお金を使ったりすることが、より高い報酬スコアにつながるかもしれません。 [クリック] Personalizer では、次にアクティビティを特集する必要があるときのために、その情報を元のランク付けシステムにフィードします。

### <a name="slide-discovering-patterns-and-causality"></a>スライド:パターンと因果関係の探索

単なるレコメンダー システムではありません。

探索モードでは、指定したレートで他のオプションを表示します。

リアルタイムの A/B テストに似ています。

### <a name="slide-personalizer-for-tailwind-traders"></a>スライド:Tailwind Traders 用の Personalizer

コンテキスト: 時刻、曜日、ブラウザー OS  

報酬スコア:特集されたカテゴリがクリックされた場合は 1。それ以外の場合は 0。

探索率:20%

### <a name="slide-demo-personalizer"></a>スライド:デモ:Personalizer

[クリック] 次に、動作中の Personalizer を見てみましょう。 Tailwind Traders ホームページに戻ります。 まだ説明していませんでしたが、この [おすすめ] セクションでは、製品部門の順序が Personalizer によって決定されます。
この例では、電気部門がヒーローのイメージとして表示されています。 WWeb サイトを何回か更新すると、"探索" 動作を確認することもできます。
現在 Personalizer では明らかに、ここで使用しているブラウザーとオペレーティング システムを 1 日のこの時間に使用する匿名のユーザーから最適なエンゲージメントを得るのは Garden Center であると考えています。しかし最終的にはさまざまなカテゴリを試します。ここでは給排水設備がポップアップ表示されました。Personalizer ではこれも使用してエンゲージメントを測定します。

### <a name="slide-pre-built-ai-in-production"></a>スライド:運用環境における事前構築済み AI

最後に、AI を運用環境に配置するための考慮事項をいくつか紹介します。

### <a name="slide-cost-considerations"></a>スライド:コストに関する考慮事項

最初の考慮事項はコストです。 

まだ Azure を使ったことがない場合は、 このリンクを使用してサインアップし、無料で $200 クレジットを取得してください。

[クリック] 開発規模のワークロードは通常無料 

[クリック] 運用のボリュームから課金開始

[クリック] サービスとリージョン別の詳細はこちらのリンクを参照

### <a name="slide-data-considerations"></a>スライド:データに関する考慮事項

データの送信先と使用方法について考えてみましょう。

データは推論用にアップロードされますが、使用後すぐに削除されます。 詳細については、こちらのリンクをご覧ください。

帯域幅が問題になっている場合、またはデータが規制されている場合は、コンテナーをご検討ください。

### <a name="slide-deployment-with-containers"></a>スライド:コンテナーを使用したデプロイ

ダウンロード可能なコンテナーで利用できるサービスもあります。

ファイアウォールの内側にコンテナーをインストールします。データが Microsoft に送信されることはありません。

インターネット接続は請求のためだけに使用されます。 通常料金で課金されます。

### <a name="slide-ethical-considerations"></a>スライド:倫理的な考慮事項

最も重要なスライドです。

ご自分の AI アプリが人々に倫理的な影響を与えることをご理解ください。

倫理的なフレームワークで考慮すべきことは以下のとおりです。

人々が既に行っている内容をより多く達成*できるようにする* (人々を置き換えない) こと

あらゆるタイプのユーザーを*含める*: すべてのユーザーがアプリケーションから等しくベネフィットを得られること、および

公平さと透明性があること。

AI は、トレーニングされたデータと同じように機能することを忘れないでください。 ご自分のアプリケーションが潜在的なユーザーすべてに対して動作することをご確認ください。

倫理的なフレームワークを設定していない場合、まずは人工知能に関する Microsoft 独自の原則をご覧いただくことをおすすめします。こちらのリンクで詳細をお読みください。

### <a name="slide-wrapping-up"></a>スライド:まとめ

事前構築済みモデルですべてを行うことはできませんが、素早く結果を出すことができます。 

AI はデータによって駆動します。 常にデータを念頭に置き、失敗しそうなものは何かを考えます。

実際に使ってみてください。 専門知識はそれほど必要ありませんが、倫理的影響についてはお考えください。

### <a name="slide-docs-alert"></a>スライド:/Docs のお知らせ

ファースト ステップ ガイドやリファレンスなど、Azure Cognitive Services の詳細については、Microsoft Docs を参照してください。

### <a name="slide-ms-learn-alert"></a>スライド:/MS Learn のお知らせ

Cognitive Services の使用方法について学習したい場合は、Microsoft Learn の無料コースをご活用ください。ステップバイステップで学ぶことができます。

### <a name="slide-resources"></a>スライド:リソース

Github リポジトリ内のすべてのリンクとコード。

AI またはデータ サイエンスにおける Microsoft 認定資格を取得したい場合は、今日の参加者のための無料の証明書を提供する特別プランをご利用ください。詳細については、こちらのリンクをご覧ください。

ご質問があれば回答させていただきます。 (そして...)

ありがとうございました。